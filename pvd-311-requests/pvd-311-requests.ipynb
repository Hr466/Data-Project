{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by: [SmirkyGraphs](http://smirkygraphs.github.io/). Code: [Github](https://github.com/SmirkyGraphs/Python-Notebooks). Source: [PVD-311](http://www.providenceri.gov/pvd-311/).\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Providence 311 Requests Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code used to scrape information from Providence's 311 platform using the api.  \n",
    "The data is then cleaned using pandas to remove blank columns and convert the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[status] page_count: 172\n"
     ]
    }
   ],
   "source": [
    "# data scraping\n",
    "url = 'https://vc0.publicstuff.com/api/2.0/requests_list?client_id=1000017&device=iframe&limit=250&'\n",
    "page_count = 1\n",
    "\n",
    "frames = []\n",
    "data = 'placeholder'\n",
    "\n",
    "while data:\n",
    "    # send request for data\n",
    "    page = str(page_count)\n",
    "    r = requests.get(url + f'page={page}') \n",
    "\n",
    "    # get the json response\n",
    "    data = json.loads(r.content)['response']['requests']\n",
    "\n",
    "    # get data into a dataframe\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df = df.to_json(orient='records')\n",
    "    df = json_normalize(json.loads(df), meta=['key'], errors='ignore')\n",
    "    frames.append(df)\n",
    "\n",
    "    # next page\n",
    "    page_count += 1\n",
    "\n",
    "    # random sleep time\n",
    "    delay = random.uniform(3,7)\n",
    "    time.sleep(delay)\n",
    "\n",
    "print(f'[status] page_count: {page_count}')\n",
    "df = pd.concat(frames, sort=False)\n",
    "df.to_csv('./data/raw/pvd_311_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "df = pd.read_csv('./data/raw/pvd_311_raw.csv', low_memory=False)\n",
    "\n",
    "# remove request. prefix for columns\n",
    "df.columns = [x[8:] for x in df.columns]\n",
    "\n",
    "# remove unwanted columns\n",
    "cols = [\n",
    "    'user_follows',\n",
    "    'user_comments',\n",
    "    'user_request',\n",
    "    'rank',\n",
    "    'primary_attachment.id',\n",
    "    'primary_attachment.extension',\n",
    "    'primary_attachment.content_type',\n",
    "    'primary_attachment.versions.small',\n",
    "    'primary_attachment.versions.medium',\n",
    "    'foreign_id'   \n",
    "]\n",
    "\n",
    "df = df.drop(columns=cols)\n",
    "\n",
    "df['date_created'] = df['date_created'].apply(lambda x: pd.Timestamp(x, unit='s', tz='US/Eastern'))\n",
    "df.to_csv('./data/clean/pvd_311_clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
