{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by: [SmirkyGraphs](http://smirkygraphs.github.io/). Code: [Github](https://github.com/SmirkyGraphs/Python-Notebooks). Source: [PVD-311](http://www.providenceri.gov/pvd-311/).\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Providence 311 Requests Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code used to create word clouds using the descriptions given in 311 requests for the 5 most commonly reported types. The word clouds are the top 200 most common words with the categories common words and stopwords removed. All words are converted into lowercase with punctuation and special characters removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_words(df):\n",
    "    \n",
    "    # getting all descriptions into a list\n",
    "    df = df[df['description'].notnull()]\n",
    "    text = df['description'].tolist()\n",
    "    \n",
    "    # lowercasing and removing extra spaces\n",
    "    text = [x.strip().lower() for x in text]\n",
    "    text = [x for x in text if x != '']\n",
    "    \n",
    "    # joining all text into a corpus\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    # removing non a-z characters\n",
    "    text = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(text, title, stop_words=None):\n",
    "    \n",
    "    # adding stopwords\n",
    "    stopwords = set(STOPWORDS)\n",
    "    \n",
    "    if stop_words != None:\n",
    "        for word in stop_words:\n",
    "            stopwords.add(word)\n",
    "\n",
    "    # setting image mask\n",
    "    mask = np.array(Image.open('./files/mask/providence-hd.png'))\n",
    "\n",
    "    # setting wordcloud params\n",
    "    wc = WordCloud(\n",
    "        font_path='./files/font/trebuc.ttf',\n",
    "        background_color='#1b1b1b',\n",
    "        max_font_size=1000,\n",
    "        mask=mask,\n",
    "        max_words=200,\n",
    "        colormap='Spectral',\n",
    "        stopwords=stopwords,\n",
    "        normalize_plurals=True,\n",
    "        random_state=1,\n",
    "        collocations=False\n",
    "    )\n",
    "\n",
    "    # generating wordcloud\n",
    "    wc = wc.generate(text)\n",
    "\n",
    "    # saving image\n",
    "    wc.to_file(f\"./output/{title}_output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data to get words by topic\n",
    "df = pd.read_csv('./data/clean/pvd_311_clean.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate wordcloud for pothole 311 Requests\n",
    "potholes = df[df['title'] == 'Potholes']\n",
    "potholes = collect_words(potholes)\n",
    "\n",
    "stop_words = ['st', 'ave', 'street', 'pothole', 'pot', 'hole', 'potholes']\n",
    "generate_wordcloud(potholes, 'potholes', stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate wordcloud for graffiti 311 Requests\n",
    "graffiti = df[df['title'] == 'Graffiti']\n",
    "graffiti = collect_words(graffiti)\n",
    "\n",
    "stop_words = ['graffiti', 'st', 'ave', 'street']\n",
    "generate_wordcloud(graffiti, 'graffiti', stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate wordcloud for trash 311 Requests\n",
    "trash = df[df['title'].str.contains('Trash')]\n",
    "trash = collect_words(trash)\n",
    "\n",
    "stop_words = ['trash', 'st', 'ave', 'street', 'nobb', 'bb', 'ticketbb']\n",
    "generate_wordcloud(trash, 'trash', stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate wordcloud for tree related 311 Requests\n",
    "trees = df[df['title'] == 'Tree Related Issues']\n",
    "trees = collect_words(trees)\n",
    "\n",
    "stop_words = ['st', 'ave', 'street', 'tree', 'trees']\n",
    "generate_wordcloud(trees, 'trees', stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate wordcloud for snow plowing 311 Requests\n",
    "snow = df[df['title'] == 'Snow Plowing, Salting or Sanding']\n",
    "snow = collect_words(snow)\n",
    "\n",
    "stop_words = ['st', 'ave', 'street', 'snow', 'plow', 'plowed', 'plowing']\n",
    "generate_wordcloud(snow, 'snow_plow', stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
